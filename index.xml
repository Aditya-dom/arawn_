<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Arawn&#39;s Blog</title>
        <link>//localhost:1313/arawn/</link>
        <description>Recent content on Arawn&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 25 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="//localhost:1313/arawn/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Harmonizing Emotions</title>
        <link>//localhost:1313/arawn/p/harmonizing-emotions/</link>
        <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/p/harmonizing-emotions/</guid>
        <description>&lt;img src="//localhost:1313/arawn/p/harmonizing-emotions/Cover.jpg" alt="Featured image of post Harmonizing Emotions" /&gt;&lt;p&gt;In the tapestry of human experience, few threads are as intricately woven as the connection between music and emotions. Across cultures and centuries, music has served as a conduit for expressing the deepest recesses of the human soul, stirring emotions that words alone often fail to capture. Yet, within the rich tapestry of Hindu mythology, this connection takes on an even more profound dimension, intertwining with the divine itself.&lt;/p&gt;
&lt;p&gt;Central to Hindu beliefs is the concept of Nada Brahman, the divine sound or cosmic vibration that permeates the universe. It is from this primordial vibration that the universe is said to have emanated, and it is within the realm of sound that the divine is most intimately experienced. In this cosmic symphony, the role of music becomes elevated to that of a sacred art, a means of communing with the divine and attuning oneself to the cosmic rhythms.&lt;/p&gt;
&lt;p&gt;At the heart of this sacred connection is Goddess Saraswati, revered as the embodiment of knowledge, music, and the arts. In Hindu iconography, she is often depicted seated on a lotus, adorned with a veena (a stringed instrument), her graceful fingers coaxing forth melodies that resonate with the very essence of creation. As the patroness of music, Saraswati&amp;rsquo;s presence infuses every note with divine inspiration, elevating music beyond mere entertainment to a form of worship.&lt;/p&gt;
&lt;p&gt;But the divine connection with music does not end with Saraswati alone. In the pantheon of Hindu deities, Lord Shiva, the supreme god of destruction and transformation, also holds a profound relationship with sound. According to Hindu mythology, Shiva is often depicted as Nataraja, the cosmic dancer whose rhythmic movements symbolize the cycles of creation, preservation, and destruction.&lt;/p&gt;
&lt;p&gt;Yet, perhaps even more intriguing is the mythological tale of Shiva&amp;rsquo;s self-creation through sound. In the ancient scriptures, it is said that Shiva, in his formless state, immersed himself in profound meditation. As he delved deeper into the depths of his being, he became aware of the primordial vibration—the eternal hum that underlies all existence. With a single sound, the divine Aum, Shiva manifested himself, transcending the boundaries of time and space to become the eternal god of creation and destruction.&lt;/p&gt;
&lt;p&gt;In this mythological narrative, the profound connection between music and divinity is laid bare. Just as Shiva created himself through the power of sound, so too does music possess the transformative ability to shape our consciousness and transcend our earthly limitations. In the rhythms and melodies that reverberate through the cosmos, we glimpse the eternal dance of creation and destruction, and in our own music-making, we participate in this divine symphony.&lt;/p&gt;
&lt;p&gt;Beyond mere entertainment or artistic expression, music becomes a sacred offering, a pathway to divine communion. Whether through the soul-stirring melodies of Saraswati&amp;rsquo;s veena or the cosmic vibrations that gave birth to Shiva himself, the divine presence infuses every note with transcendental significance. And as we listen, create, and immerse ourselves in the mystical world of sound, we find ourselves drawn ever closer to the divine source from which all music emanates.&lt;/p&gt;
&lt;p&gt;In the end, the connection between music, human emotions, and Hindu mythology is not merely a matter of cultural tradition or religious belief. It is a profound reminder of the inherent unity of all existence, a testament to the power of sound to transcend the limitations of the material world and connect us to the eternal rhythms of the cosmos. And in this divine symphony, we find solace, inspiration, and a glimpse of the ineffable beauty that lies at the heart of creation itself.&lt;/p&gt;
&lt;script src=&#34;https://giscus.app/client.js&#34;
        data-repo=&#34;Aditya-dom/arawn.github.io&#34;
        data-repo-id=&#34;R_kgDOLeAbmQ&#34;
        data-category=&#34;General&#34;
        data-category-id=&#34;DIC_kwDOLeAbmc4CeCQd&#34;
        data-mapping=&#34;title&#34;
        data-strict=&#34;0&#34;
        data-reactions-enabled=&#34;1&#34;
        data-emit-metadata=&#34;1&#34;
        data-input-position=&#34;top&#34;
        data-theme=&#34;dark_dimmed&#34;
        data-lang=&#34;en&#34;
        data-loading=&#34;lazy&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;</description>
        </item>
        <item>
        <title>Love Struck</title>
        <link>//localhost:1313/arawn/p/love-struck/</link>
        <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/p/love-struck/</guid>
        <description>&lt;img src="//localhost:1313/arawn/p/love-struck/love.jpeg" alt="Featured image of post Love Struck" /&gt;&lt;h2 id=&#34;love&#34;&gt;
    &lt;a href=&#34;#love&#34;&gt;#&lt;/a&gt;
    ¿Love?
&lt;/h2&gt;&lt;script src=&#34;https://giscus.app/client.js&#34;
        data-repo=&#34;Aditya-dom/arawn.github.io&#34;
        data-repo-id=&#34;R_kgDOLeAbmQ&#34;
        data-category=&#34;General&#34;
        data-category-id=&#34;DIC_kwDOLeAbmc4CeCQd&#34;
        data-mapping=&#34;title&#34;
        data-strict=&#34;0&#34;
        data-reactions-enabled=&#34;1&#34;
        data-emit-metadata=&#34;1&#34;
        data-input-position=&#34;top&#34;
        data-theme=&#34;dark_dimmed&#34;
        data-lang=&#34;en&#34;
        data-loading=&#34;lazy&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;</description>
        </item>
        <item>
        <title>Setting up my Mac</title>
        <link>//localhost:1313/arawn/p/mac-system-setup/</link>
        <pubDate>Thu, 14 Mar 2024 22:48:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/p/mac-system-setup/</guid>
        <description>&lt;img src="//localhost:1313/arawn/p/mac-system-setup/setup.jpeg" alt="Featured image of post Setting up my Mac" /&gt;&lt;h1 id=&#34;system-setup-log&#34;&gt;
    &lt;a href=&#34;#system-setup-log&#34;&gt;#&lt;/a&gt;
    System Setup Log
&lt;/h1&gt;&lt;p&gt;As an avid user of the MacBook Pro 13 (2022), I am thrilled to share my enthusiastic endorsement of this remarkable device. From its sleek and elegant design to its unparalleled performance and innovative features, the MacBook Pro 13 (2022) has exceeded my expectations on every front.&lt;/p&gt;
&lt;p&gt;Overall, the MacBook Pro 13 (2022) has truly redefined my expectations of what a laptop can be. With its unparalleled performance, stunning display, and innovative features, it&amp;rsquo;s the perfect companion for anyone looking to unleash their creativity, boost their productivity, and elevate their computing experience to new heights.
I set this machine up, I did a fullhearted job of logging the process.&lt;/p&gt;
&lt;h2 id=&#34;the-procedure&#34;&gt;
    &lt;a href=&#34;#the-procedure&#34;&gt;#&lt;/a&gt;
    The Procedure
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;User Settings
&lt;ul&gt;
&lt;li&gt;System came with my home directory set as &amp;lsquo;aditya&amp;rsquo; and User name as
&amp;lsquo;Arawn&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Create a new admin account, log out of current account and into admin account&lt;/li&gt;
&lt;li&gt;In admin account, rename home directory to &amp;lsquo;Sensei&amp;rsquo; (or whatever name),
and then in Preferences-&amp;gt;Users and Groups, unlock and ctrl-click on your
account for &amp;lsquo;advanced settings&amp;rsquo;&lt;/li&gt;
&lt;li&gt;In advanced settings, change the name to your name and name of the home
directory to the name of your home directory. Make sure user name is same
as that of home directory.&lt;/li&gt;
&lt;li&gt;Restart, log back in and delete admin account.&lt;/li&gt;
&lt;li&gt;Set the computer HostName, LocalHostName and ComputerName to MacBook-Pro:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo scutil --set HostName MacBook-Pro  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo scutil --set LocalHostName MacBook-Pro  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo scutil --set ComputerName MacBook-Pro  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dscacheutil -flushcache  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;restart&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;System Prefs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resolution of 1440x900 (Displays)&lt;/li&gt;
&lt;li&gt;Uncheck natural scroll direction (Trackpad-&amp;gt;Scroll and Zoom)&lt;/li&gt;
&lt;li&gt;Use F1, F2 as function keys (Keyboard)&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t show input menu in menu bar (Keyboard-&amp;gt;Input Sources)&lt;/li&gt;
&lt;li&gt;Dark Mode&lt;br&gt;
&lt;img src=&#34;//localhost:1313/arawn/arawn/p/mac-system-setup/darkside.jpeg&#34;
	width=&#34;271&#34;
	height=&#34;186&#34;
	srcset=&#34;//localhost:1313/arawn/arawn/p/mac-system-setup/darkside_hu98a3c37bcd80352aa85b64c6da9eb82d_9502_480x0_resize_q75_box.jpeg 480w, //localhost:1313/arawn/arawn/p/mac-system-setup/darkside_hu98a3c37bcd80352aa85b64c6da9eb82d_9502_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;dark side meme&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;349px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Change computer name to MacBook-Pro (Sharing)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Google Chrome:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download, drag and drop&lt;/li&gt;
&lt;li&gt;Sign in and sync stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sublime Text&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download, drag and drop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Content&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transfer personal files from HDD&lt;/li&gt;
&lt;li&gt;Screen background to &lt;code&gt;hacker_img.jpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Things 3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install from Mac App Store&lt;/li&gt;
&lt;li&gt;Sign in and sync tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iTerm 2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download, drag and drop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By now, the system should be looking pretty good. Catalina uses &lt;code&gt;zsh&lt;/code&gt; rather than
bash, and clang rather than gcc, so porting some stuff over will be tricky, but
we&amp;rsquo;ll get to that in a minute.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Git
&lt;ul&gt;
&lt;li&gt;download mac installer from &lt;code&gt;git-scm.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;ctrl-click and click &amp;lsquo;open&amp;rsquo; (security won&amp;rsquo;t let you click to open)&lt;/li&gt;
&lt;li&gt;follow installer guidelines
? Will xcode command line tools eat this up?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;lt;DON&amp;rsquo;T DO THIS!&amp;gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python
&lt;ul&gt;
&lt;li&gt;Download latest stable installer from web&lt;/li&gt;
&lt;li&gt;Double click and install &lt;br&gt;
! Don&amp;rsquo;t do this because this does not install python properly; you have to
hack around it and do a ton of exports and stuff. I had to uninstall
python after this, using instructions from &lt;a class=&#34;link&#34; href=&#34;https://stackoverflow.com/questions/3819449/how-to-uninstall-python-2-7-on-a-mac-os-x-10-6-4/3819829#381982&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;
and &lt;a class=&#34;link&#34; href=&#34;https://superuser.com/questions/276840/uninstalling-python-3-on-a-mac&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.
Best way to install is to use &lt;code&gt;brew&lt;/code&gt; (this keeps the python updated) and
&lt;code&gt;pyenv&lt;/code&gt;, so first install &lt;code&gt;brew&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;lt;/DON&amp;rsquo;T DO THIS!&amp;gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;XCode Command Line Tools&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCC, &lt;code&gt;clang&lt;/code&gt;, &lt;code&gt;make&lt;/code&gt; etc etc&lt;/li&gt;
&lt;li&gt;Download from developer.apple.com, mount, use .pkg installer&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s a 400 MB download and takes 2.54 GB of space on the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;This doesn&amp;rsquo;t eat up git (&lt;code&gt;git --version&lt;/code&gt; still gives git 2.27, so yay!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Homebrew&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Needed for practically everything :P&lt;/li&gt;
&lt;li&gt;visit brew.sh and copy-paste the installation command&lt;/li&gt;
&lt;li&gt;Again, a huge download here (350 MB)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;pyenv&lt;/code&gt; to download and install python, as shown in the article &lt;a class=&#34;link&#34; href=&#34;https://opensource.com/article/19/5/python-3-default-mac&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://opensource.com/article/19/5/python-3-default-mac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In short, do the following:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;brew install pyenv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pyenv install 3.9.0 &lt;span class=&#34;c1&#34;&gt;# replace with latest python version&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pyenv global 3.9.0 &lt;span class=&#34;c1&#34;&gt;# set 3.9.0 as global version&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pyenv version &lt;span class=&#34;c1&#34;&gt;# double check version&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; -e &lt;span class=&#34;s1&#34;&gt;&amp;#39;if command -v pyenv 1&amp;gt;/dev/null 2&amp;gt;&amp;amp;1; then\n  eval &amp;#34;$(pyenv init -)&amp;#34;\nfi&amp;#39;&lt;/span&gt; &amp;gt;&amp;gt; ~/.zshrc &lt;span class=&#34;c1&#34;&gt;# voodoo magic that allows pyenv to work&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;and now do &lt;code&gt;python -V&lt;/code&gt; and &lt;code&gt;pip -V&lt;/code&gt; to verify that python 3.9.0 is loaded&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vim&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy &lt;code&gt;.vim&lt;/code&gt; and &lt;code&gt;.vimrc&lt;/code&gt; from previous system&lt;/li&gt;
&lt;li&gt;check if clipboard copy/paste is enabled: &lt;code&gt;vim --version | grep clipboard&lt;/code&gt;,
a + sign appears if clipboard is enabled&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ZSH&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edit &lt;code&gt;.zprofile&lt;/code&gt; and add &lt;code&gt;export CLICOLOR=1&lt;/code&gt; for coloured output from commands&lt;/li&gt;
&lt;li&gt;add prompt tweaks &lt;code&gt;PS1=&#39;%F{green}%n@%m:%f %F{blue}%~%f %% &#39;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iTerm2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tweak profile; first, create a new profile&lt;/li&gt;
&lt;li&gt;Change background color to &lt;code&gt;333333&lt;/code&gt;, foreground to &lt;code&gt;dddddd&lt;/code&gt; and other colors
to pastel colours&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mail&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Log in to google&lt;/li&gt;
&lt;li&gt;Configure signatures, calendars, mail folders etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Projects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the meaty part. Start with the blog:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone --select-branch --branch &lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; https://github.com/Aditya-dom/arawn.github.io 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mv arawn.github.io blog
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; blog
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;virtualenv venv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; venv/bin/activate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;venv&amp;#34;&lt;/span&gt; &amp;gt; .gitignore
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -r requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;then whip up this article. I ran into a small hiccup while using &lt;code&gt;make html&lt;/code&gt;:
the version of Pelican that &lt;code&gt;pip&lt;/code&gt; installed was 4.5.0, and that broke with the
old Flex theme. I had made some modifications to the theme (MathJax) and didn&amp;rsquo;t
want to lose them by upgrading. So, I downgraded pelican to 4.2.0 and then made
the files. Takeaway is use &lt;code&gt;make html DEBUG=1&lt;/code&gt; for debugger output. Do a &lt;code&gt;make serve&lt;/code&gt;
to check out formatting, after which do &lt;code&gt;make github&lt;/code&gt; to push to github pages, which is what
you&amp;rsquo;re reading now (this article has been made from the mac :)&lt;/p&gt;
&lt;p&gt;for further updates, refer to the repository linked above.&lt;/p&gt;
&lt;script src=&#34;https://giscus.app/client.js&#34;
        data-repo=&#34;Aditya-dom/arawn.github.io&#34;
        data-repo-id=&#34;R_kgDOLeAbmQ&#34;
        data-category=&#34;General&#34;
        data-category-id=&#34;DIC_kwDOLeAbmc4CeCQd&#34;
        data-mapping=&#34;title&#34;
        data-strict=&#34;0&#34;
        data-reactions-enabled=&#34;1&#34;
        data-emit-metadata=&#34;1&#34;
        data-input-position=&#34;top&#34;
        data-theme=&#34;dark_dimmed&#34;
        data-lang=&#34;en&#34;
        data-loading=&#34;lazy&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;</description>
        </item>
        <item>
        <title>Optimizers, Part 1</title>
        <link>//localhost:1313/arawn/p/optimizers-1/</link>
        <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/p/optimizers-1/</guid>
        <description>&lt;p&gt;Happy Holi! This &lt;strike&gt;is going to&lt;/strike&gt; was supposed to be a long
one, so sit back and grab a &lt;a class=&#34;link&#34; href=&#34;https://media-assets.swiggy.com/swiggy/image/upload/fl_lossy,f_auto,q_auto,w_300,h_300,c_fit/uctifimyzfngegnzujrc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;chocolate gujia&lt;/a&gt; (and preferably view this on your laptop)&lt;/p&gt;
&lt;center&gt;
&lt;iframe src=&#34;intro_plot.html&#34; style=&#34;width: 100%; height: 650px; border: 0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;Some optimization algorithms. Click on a colour in the legend to hide/show it&lt;/p&gt;
&lt;/center&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;
    &lt;a href=&#34;#table-of-contents&#34;&gt;#&lt;/a&gt;
    Table of Contents
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solution-existence&#34;&gt;Do Solutions Even Exist?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#structure&#34;&gt;How this guide is structured&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-descent-optimizers&#34;&gt;Gradient Descent Optimizers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sgd-with-momentum&#34;&gt;SGD with Momentum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sgd-with-nesterov-momentum&#34;&gt;SGD with Nesterov Momentum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-descent-comparision&#34;&gt;Putting it all together&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#refs-and-footnotes&#34;&gt;References and Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
3. &lt;a href=&#34;#adaptive-optimizers&#34;&gt;Adaptive Optimizers&lt;/a&gt;
    * &lt;a href=&#34;#adagrad&#34;&gt;AdaGrad&lt;/a&gt;
    * &lt;a href=&#34;#rmsprop&#34;&gt;RMSProp&lt;/a&gt;
    * &lt;a href=&#34;#rmsprop-with-nesterov-momentum&#34;&gt;RMSProp with Nesterov Momentum&lt;/a&gt;
    * &lt;a href=&#34;#adam&#34;&gt;Adam&lt;/a&gt;
    * &lt;a href=&#34;#nadam-nesterov-adam&#34;&gt;NAdam (Nesterov Adam)&lt;/a&gt;
    * &lt;a href=&#34;#adamw&#34;&gt;AdamW&lt;/a&gt;
    * &lt;a href=&#34;#amsgrad&#34;&gt;AMSGrad&lt;/a&gt;
    * &lt;a href=&#34;#adabound&#34;&gt;AdaBound&lt;/a&gt;
    * &lt;a href=&#34;#adabelief&#34;&gt;AdaBelief&lt;/a&gt;
4. &lt;a href=&#34;#second-order-optimizers&#34;&gt;Second Order Optimizers&lt;/a&gt;
    * &lt;a href=&#34;#newton&#34;&gt;Newton&lt;/a&gt;
    * &lt;a href=&#34;#conjugate-gradients&#34;&gt;Conjugate Gradients&lt;/a&gt;
        * &lt;a href=&#34;#fletcher-reeves&#34;&gt;Fletcher-Reeves&lt;/a&gt;
        * &lt;a href=&#34;#polak-ribiere&#34;&gt;Polak-Ribiere&lt;/a&gt;
    * &lt;a href=&#34;#bfgs&#34;&gt;BFGS&lt;/a&gt;
    * &lt;a href=&#34;#l-bfgs&#34;&gt;L-BFGS&lt;/a&gt;--&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Most supervised learning tasks involve optimizing a loss function, in order to
fit the model to the given training data. Of these, the most common is neural
network training: neural networks have millions (even billions) of parameters
which need to be tuned so that the model can predict the right outputs.&lt;/p&gt;
&lt;p&gt;Obtaining closed form solutions to neural network problems is more often than
not intractable, and so we perform this optimization algorithmically and
numerically. The main things we look for in an algorithm that optimizes the
loss function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It should converge&lt;/strong&gt;: Sounds like a no-brainer, but the algorithm should be
able to decide when and where to stop, and to ensure that the location at
which it stops is a local minima.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It should converge quickly&lt;/strong&gt;: The algorithm should take as few steps as
possible to converge, as every step requires a parameter update, and updating
millions (if not billions) of parameters is inefficient. Therefore, it should
take the optimal steps at every point.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It should be able to deal with ambiguity&lt;/strong&gt;: Loosely worded, the algorithm
would not have the exact value of the gradient: all the algorithms described
use a batch of samples to obtain an estimate of the gradient, and the
expected value of the gradient obtained should equal the gradient of the
function at this point. The algorithm should be able to converge to a local
minima even if it obtains incorrect gradients at some steps in the process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution-existence&#34;&gt;Do solutions even exist?&lt;/h2&gt;
&lt;p&gt;We can&amp;rsquo;t really go further without knowing what the loss landscape looks like:
do solutions even exist? How do we visualize a high-dimensional loss landscape?&lt;/p&gt;
&lt;p&gt;A few observations about the loss landscape of a neural network from Goodfellow
are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;As dimensionality of the latent space increases, the probability a
critical point is a local minima decreases&lt;/strong&gt;. The curvature of a point is
given by the eigenvalues of the hessian: if all the eigenvalues are positive,
the point is a local minima (and the hessian is positive semi-definite), and
the opposite for a local maxima. If we consider a random function, choosing
the sign of the eigenvalue is akin to tossing a coin. Therefore, in
n-dimensional space, if we toss n coins to determine these signs, the
probability that all of them turn up positive is very low. Therefore, high-
dimensional space has more saddle points than local minima/maxima&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;There are several equally good local minima&lt;/strong&gt;: because of scale invariance,
if you scale the inputs to a layer down by 10 and multiply the outputs by 10,
then you get the same resultant network. Also, if you switch the position of
two neurons in a layer with each other while maintaining the connections,
you get the same network. These two similarities result in a large number of
similar optima, reaching any one of which will optimize the entire network.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these are in our favour, and show us that reaching a local minima in
high-dimensional space should be sufficient to fit the network. We&amp;rsquo;ll now take
a look at some algorithms which do this.&lt;/p&gt;
&lt;p&gt;As for visualizing the loss landscape, this is significantly trickier to do.
This work by &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1712.09913v3.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Goldstein et al&lt;/a&gt; does a
good job of it, but visualizing and comparing the paths taken by optimization
algorithms on this landscape is very difficult: because what we&amp;rsquo;re seeing is a
projection onto two dimensions, the direction taken by the path need not
correspond to the direction of maximum descent. This repository by &lt;a class=&#34;link&#34; href=&#34;https://github.com/logancyang/loss-landscape-anim&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Logan Yang&lt;/a&gt;
had a good implementation of this paper, along with traces of the paths taken
by various optimization algorithms showing why we can&amp;rsquo;t use this to
qualitatively compare different optimization algorithms with each other&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;loss_landscape_goldstein.png&#34; width=800px&gt;&lt;/img&gt;
&lt;p&gt;The loss landscape of ResNet-56 (source: Goldstein et al)&lt;/p&gt;
&lt;/center&gt;
&lt;h2 id=&#34;structure&#34;&gt;How this guide is structured&lt;/h2&gt;
&lt;p&gt;While most deep learning problems use a super high-dimensional loss function,
for the purposes of this guide we&amp;rsquo;ll use a simple 2-D loss function which is
a linear combination of gaussians of the following form
$$\begin{gather}
f(x, y) = se^{-(ax+by+c)^2} \\
\mathcal{L} = \sum_{i=1}^{n} f_i(x, y)
\end{gather}$$&lt;/p&gt;
&lt;p&gt;The good thing about gaussians is that they&amp;rsquo;re easy to differentiate
$$\begin{align}
\frac{\partial f}{\partial x} &amp;amp;= -2a(ax+by+c)f(x,y) \\
\frac{\partial f}{\partial y} &amp;amp;= -2b(ax+by+c)f(x,y) \\
\frac{\partial^2 f}{\partial x^2} &amp;amp;= -2a^2(1 - 2(ax+by+c)^2)f(x,y) \\
\frac{\partial^2 f}{\partial x \partial y} &amp;amp;= -2ab(1 - 2(ax+by+c)^2)f(x,y) \\
\frac{\partial^2 f}{\partial y^2} &amp;amp;= -2b^2(1 - 2(ax+by+c)^2)f(x,y)
\end{align}$$&lt;/p&gt;
&lt;p&gt;so all the methods can use the exact gradient/hessian of the loss function
rather than a &lt;em&gt;stochastic&lt;/em&gt; one. This is kind of cheating, but since this is a
science experiment, let&amp;rsquo;s run with it. about this loss func&lt;/p&gt;
&lt;p&gt;$s, a, b, c$ are generated uniform randomly from a suitable range, and I played
around manually with this till I got a loss function that looked funky enough
for my needs. We finally use the following loss function, and it&amp;rsquo;s been
exported to the file &lt;code&gt;func.dill&lt;/code&gt; if you want to load it in (use dill to load it,
as there were some errors serializing it via pickle)&lt;/p&gt;
&lt;iframe src=&#34;loss_fn_interactive.html&#34; style=&#34;width: 100%; height: 620px; border: 0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;The convergence criterion that&amp;rsquo;s used for all optimizers is when the gradient norm
is less than 0.05, and all the optimizers are limited to take atmost 1000 steps.&lt;/p&gt;
&lt;p&gt;The plots are made in Bokeh/Plotly and are interactive (if you haven&amp;rsquo;t already
played with the plot we generated in the start). I&amp;rsquo;ve done my best to be
inspired by &lt;a class=&#34;link&#34; href=&#34;https://distill.pub&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Distill&lt;/a&gt;, most notably &lt;a class=&#34;link&#34; href=&#34;https://distill.pub/2017/momentum/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Gabriel Goh&lt;/a&gt;&amp;rsquo;s
beautiful, interactive article on momentum.&lt;/p&gt;
&lt;h1 id=&#34;gradient-descent-optimizers&#34;&gt;Gradient Descent Optimizers&lt;/h1&gt;
&lt;p&gt;Gradient Descent optimizers converge to a local minimum by simply following the
gradient: there&amp;rsquo;s no adaptiveness here, and it&amp;rsquo;s akin to feeling the area
around your feet and just taking a small step in the steepest direction, and
repeating that till you get to the minima. There are a few tricks here and we
take hints from Physics to speed up the convergence, but most of the algorithm
relies on the gradient, and the speed with which we&amp;rsquo;re already going.&lt;/p&gt;
&lt;h2 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h2&gt;
&lt;p&gt;SGD is probably the first optimization algorithm one thinks of. It&amp;rsquo;s
deceptively simple: Look around and take a step in the direction where the
gradient decreases the most. Once you&amp;rsquo;ve taken the step, &lt;strong&gt;Stop&lt;/strong&gt;, look around
again, and repeat until you&amp;rsquo;re at the minima (the gradient is sufficiently
small or you come back to a point you&amp;rsquo;ve visited).&lt;/p&gt;
&lt;p&gt;Th &lt;em&gt;S&lt;/em&gt; in SGD comes from the fact that the gradient that the algorithm obtains
in practice is not perfect: it&amp;rsquo;s an approximation of the actual gradient of the
loss function, based on the batch of examples that are sampled. However, this
approximates the gradient reasonably well, and in the long run, the expected
path taken by this algorithm comes out to be the same as the path taken when we
can perfectly obtain the gradient.&lt;/p&gt;
&lt;p&gt;The update rule for SGD is fairly simple:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\theta_{t+1} &amp;amp;\leftarrow \theta_{t} - \epsilon g(\theta_{t})
\end{align}$$&lt;/p&gt;
&lt;p&gt;Combining this with a convergence criterion gives us the algorithm (implemented
in python here)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;SGD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;5e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;5e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;sgd-with-momentum&#34;&gt;SGD with Momentum&lt;/h2&gt;
&lt;p&gt;While SGD is simple, it is slow to converge, taking several more steps than
required. This is because we come to a stop once we take a step, and the size
of the next step is solely determined by the gradient at that point. This means
that if we&amp;rsquo;re in a long stretch where the gradient is small, we can only
descend at the speed $\epsilon g$, even though we know that the stretch is
reasonably long. This slows down our algorithm and makes it take a longer time
to converge.&lt;/p&gt;
&lt;p&gt;Momentum counters this by providing some inertia to the process. Intuitively,
if SGD is a person stopping and taking a step in the direction of maximum
descent, momentum is equivalent to throwing a ball down the incline of a given
mass and seeing where it settles. If you take a look at the path momentum
follows in the introduction plot, you can see that it doesn&amp;rsquo;t immediately stop
when it comes to a point with a zero (or very small) gradient; instead, it
oscillates until it loses all it&amp;rsquo;s velocity.&lt;/p&gt;
&lt;p&gt;How do we simulate adding &amp;lsquo;mass&amp;rsquo; to the update steps? We claim that the ball
moves at a velocity $v$, and model $v$ as an exponential moving average of the
current velocity and the gradient at the current point. The update equations
are as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
v_{t+1} &amp;amp;\leftarrow \alpha v_{t} - \epsilon g(\theta_{t}) \\
\theta_{t+1} &amp;amp;\leftarrow \theta_{t} + v_{t+1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s the maximum velocity we can move at? If all the gradients are in the
same direction for an infinite (practically a very large) period of time, then
this velocity is equal to&lt;/p&gt;
&lt;p&gt;$$v_{\text{max}} = \frac{\epsilon g}{1-\alpha}$$&lt;/p&gt;
&lt;p&gt;This can be derived by expanding out the recurrence in the update step, to
obtain an infinite GP. This GP converges when $\alpha &amp;lt; 1$ to $1/(1-\alpha)$.
We can think of $1-\alpha$ as the &amp;lsquo;mass&amp;rsquo; of the ball: the smaller this quantity
is, the faster the ball will move.&lt;/p&gt;
&lt;p&gt;Generally (and in this simulation as well), $\alpha = 0.9$, so $1-\alpha = 0.1$.
This means that we can move atmost ten times faster than the step size at a
point, and this is what causes momentum to converge faster!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;SGD_momentum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;5e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;5e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;What do these gradient updates look like in practice? For starters, all changes
in the direction of the path are caused due to changes in the gradient. Where
the path takes a turn, the gradient is normal or antiparallel to the current
velocity, and at places where the path is straight, both the gradient and the
velocity are parallel. We can draw out the update vectors at two points in the
path above to see how this works.&lt;/p&gt;
&lt;iframe src=&#34;momentum_vectors.html&#34; style=&#34;width: 100%; height: 650px; border: 0&#34;&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;sgd-with-nesterov-momentum&#34;&gt;SGD with Nesterov Momentum&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;ve seen the path that momentum takes, there is one issue: &lt;em&gt;Momentum
doesn&amp;rsquo;t stop very soon&lt;/em&gt;. It&amp;rsquo;s easy to get the ball rolling, but harder to stop
it. This happens because the gradient that&amp;rsquo;s added to the path is the gradient
&lt;em&gt;at the current point&lt;/em&gt;, not the gradient &lt;em&gt;at the point at which we would have
been, if we took the step&lt;/em&gt;. In a continuous, real-world scenario, this
difference is infinitesimal, but in a numerical scenario, it becomes
significant if our step size is not small enough. This is also not an issue if
our gradients at consecutive points are similar, but becomes an issue if we
&amp;lsquo;jump&amp;rsquo; across a local minima: momentum would push us even further forward,
because the gradient at the current point is downward.&lt;/p&gt;
&lt;p&gt;This &amp;lsquo;bug&amp;rsquo; was discovered by Nesterov, and the fix was to compute the gradient
at $\theta_{t} + \alpha v_{t}$ (the position we will be at, if the gradient is
zero) rather than at $\theta_{t}$ (our current position). This &amp;lsquo;pulls&amp;rsquo; the
gradient back if we jump across a minima&lt;/p&gt;
&lt;iframe src=&#34;nesterov_comparision.html&#34; style=&#34;width: 100%; height: 650px; border: 0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;The implementation and update are quite similar, with just a minor update to
the gradient calculation.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
v_{t+1} &amp;amp;\leftarrow \alpha v_{t} - \epsilon g(\theta_{t} + \alpha v_{t}) \\
\theta_{t+1} &amp;amp;\leftarrow \theta_{t} + v_{t+1}
\end{align}$$&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;SGD_nesterov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;L&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;5e-2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;gradient-descent-comparision&#34;&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s an interactive demo, showing the paths taken by SGD, SGD with Momentum
and SGD with Nesterov Updates. The arrows have the same colour scheme as
before, and show the directions in which the path is pulled (their sum is the
next resultant step). Playing around with this should give you an idea of how
these algorithms update themselves&lt;/p&gt;
&lt;iframe src=&#34;comparision_plot.html&#34; style=&#34;width: 100%; height: 650px; border: 0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;Even though there are a large number of new algorithms for optimization, SGD
with Nesterov momentum (along with Adam) remains the algorithm of choice for
training very large neural networks: it&amp;rsquo;s stable, explainable and converges
nicely.&lt;/p&gt;
&lt;!--
&lt;h2 id=&#34;adaptive-optimizers&#34;&gt;Adaptive Optimizers&lt;/h2&gt;


&lt;h2 id=&#34;adagrad&#34;&gt;AdaGrad&lt;/h2&gt;

$$\begin{align}
r_{t+1} &amp;\leftarrow r_{t} + (g(\theta_{t}))^2 \\\\
v_{t+1} &amp;\leftarrow -\frac{\epsilon g(\theta_{t})}{\sqrt{r_{t+1} + \delta}} \\\\
\theta_{t+1} &amp;\leftarrow \theta_{t} + v_{t+1}
\end{align}$$

```python
def AdaGrad(L, G, p0, eps=5e-2):
    r = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    while norm(g) &gt; 5e-2:
        g = G(p)
        r = r + g**2
        v = - ((eps)/np.sqrt(r))*g
        p = p + v
```

&lt;h2 id=&#34;rmsprop&#34;&gt;RMSProp&lt;/h2&gt;

$$\begin{align}
r_{t+1} &amp;\leftarrow \rho r_{t} + (1-\rho)(g(\theta_{t}))^2 \\\\
v_{t+1} &amp;\leftarrow -\frac{\epsilon g(\theta_{t})}{\sqrt{r_{t+1} + \delta}} \\\\
\theta_{t+1} &amp;\leftarrow \theta_{t} + v_{t+1}
\end{align}$$

```python
def RMSProp(L, G, p0, eps=1e-2, decay=0.9):
    r = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    while norm(g) &gt; 5e-2:
        g = G(p)
        r = decay*r + (1-decay)*g**2
        v = - ((eps)/np.sqrt(r+1e-7))*g
        p = p + v
```

&lt;h2 id=&#34;rmsprop-with-nesterov-momentum&#34;&gt;RMSProp with Nesterov Momentum&lt;/h2&gt;

$$\begin{align}
r_{t+1} &amp;\leftarrow \rho r_{t} + (1-\rho)(g(\theta_{t} + \alpha v_{t}))^2 \\\\
v_{t+1} &amp;\leftarrow \alpha v_{t} - \frac{\epsilon g(\theta_{t})}{\sqrt{r_{t+1} + \delta}} \\\\
\theta_{t+1} &amp;\leftarrow \theta_{t} + v_{t+1}
\end{align}$$

```python
def RMSProp_nesterov(L, G, p0, eps=1e-2, decay=0.9, a=0.9):
    r = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    while norm(g) &gt; 5e-2:
        g = G(p + a*v)
        r = decay*r + (1-decay)*g**2
        v = a*v - ((eps)/np.sqrt(r+1e-7))*g
        p = p + v
```

&lt;h2 id=&#34;adam&#34;&gt;Adam&lt;/h2&gt;

```python
def Adam(L, G, p0, eps=1e-3, b1=0.9, b2=0.999):
    r = np.zeros(2)
    s = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    t = 0
    while norm(g) &gt; 5e-2:
        g = G(p)
        t += 1
        s = b1*s + (1-b1)*g
        r = b2*r + (1-b2)*g**2
        sh = s/(1-b1**t)
        rh = r/(1-b2**t)
        v = - ((eps)/np.sqrt(rh+1e-7))*sh
        p = p + v
```

&lt;h2 id=&#34;nadam-nesterov-adam&#34;&gt;NAdam (Nesterov Adam)&lt;/h2&gt;

```python
def NAdam(L, G, p0, eps=1e-3, b1=0.9, b2=0.999):
    r = np.zeros(2)
    s = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    t = 0
    while norm(g) &gt; 5e-2:
        g = G(p)
        t += 1
        s = b1*s + (1-b1)*g
        r = b2*r + (1-b2)*g**2
        sh = b1*s/(1-b1**t) + (1-b1)*g/(1-b1**t) # nesterov step
        rh = r/(1-b2**t)
        v = - ((eps)/np.sqrt(rh+1e-7))*sh
        p = p + v
```

&lt;h2 id=&#34;adamw&#34;&gt;AdamW&lt;/h2&gt;

```python
def AdamW(L, G, p0, eps=1e-3, b1=0.9, b2=0.999, l=1):
    r = np.zeros(2)
    s = np.zeros(2)
    p = p0
    v = 0
    g = np.inf
    t = 0
    while norm(g) &gt; 5e-2:
        g = G(p)
        t += 1
        s = b1*s + (1-b1)*g
        r = b2*r + (1-b2)*g**2
        sh = s/(1-b1**t)
        rh = r/(1-b2**t)
        v = - ( ((eps)/np.sqrt(rh+1e-7))*sh + l*p ) # adamW step
        p = p + v
```

&lt;h2 id=&#34;amsgrad&#34;&gt;AMSGrad&lt;/h2&gt;


&lt;h2 id=&#34;adabound&#34;&gt;AdaBound&lt;/h2&gt;


&lt;h2 id=&#34;adabelief&#34;&gt;AdaBelief&lt;/h2&gt;


&lt;h1 id=&#34;second-order-optimizers&#34;&gt;Second Order Optimizers&lt;/h1&gt;


&lt;h2 id=&#34;newton&#34;&gt;Newton&lt;/h2&gt;


&lt;h2 id=&#34;conjugate-gradients&#34;&gt;Conjugate Gradients&lt;/h2&gt;


&lt;h3 id=&#34;fletcher-reeves&#34;&gt;Fletcher-Reeves&lt;/h3&gt;


&lt;h3 id=&#34;polak-ribiere&#34;&gt;Polak-Ribiere&lt;/h3&gt;


&lt;h2 id=&#34;bfgs&#34;&gt;BFGS&lt;/h2&gt;


&lt;h2 id=&#34;l-bfgs&#34;&gt;L-BFGS&lt;/h2&gt;--&gt;
&lt;h1 id=&#34;refs-and-footnotes&#34;&gt;References and Footnotes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Goh, &amp;ldquo;Why Momentum Really Works&amp;rdquo;, Distill, 2017.
&lt;a class=&#34;link&#34; href=&#34;http://doi.org/10.23915/distill.00006&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://doi.org/10.23915/distill.00006&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Goodfellow, Ian, Bengio, Yoshua and Courville, Aaron. Deep Learning. : MIT
Press, 2016.&lt;/li&gt;
&lt;li&gt;Melville, James. Nesterov Accelerated Gradient and Momentum.
&lt;a class=&#34;link&#34; href=&#34;https://jlmelville.github.io/mize/nesterov.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://jlmelville.github.io/mize/nesterov.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;This was supposed to also feature adaptive optimizers (AMSGrad, RMSProp, Adam
and friends), but due to CCIC happening in the last week of March, I didn&amp;rsquo;t
get the time to do this properly, and the second semester started &lt;strike&gt; a few days ago :/ &lt;/strike&gt; tomorrow, so hard deadline :/ I&amp;rsquo;ll try to get part 2 out
as soon as possible, but it might be a while. In the meantime, exploring the
source might help for the impatient.&lt;/p&gt;
&lt;p&gt;For the complete code, and to play around and implement your own optimizers,
The repository will attached soon.
Stay Connected.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>About</title>
        <link>//localhost:1313/arawn/about/</link>
        <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/about/</guid>
        <description>&lt;p&gt;Hello, I&amp;rsquo;m Aditya, a first-year undergraduate student at IET Lucknow. My academic journey is marked by a keen interest in cutting-edge technologies, particularly within the realms of Deep Learning and Quantitative Research and Trading. Currently, I am engaged in independent research on Large Language Models (LLMs) using platforms like &lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hugging Face&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://openai.com/research/gpt-4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenAI&amp;rsquo;s GPT models (GPT-4 Turbo)&lt;/a&gt;, including &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library/llama2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ollama.v.llama2&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;In addition to my academic pursuits, I bring a practical dimension to my passion through hands-on experiences. I have delved into the world of algorithmic trading with a focus on quantitative research &amp;amp; trading. My exposure includes valuable experience at &lt;a class=&#34;link&#34; href=&#34;https://density.exchange/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Density Exchange&lt;/a&gt;, where I actively participated in algorithmic trading. This experience has not only sharpened my analytical skills but has also given me insights into the dynamic landscape of financial markets.&lt;/p&gt;
&lt;p&gt;My passion extends beyond the theoretical, as I actively explore the practical applications of my knowledge. I have a particular inclination towards Web Development, seeking to blend creativity with functionality. Additionally, I find joy and challenge in Competitive Coding, where I continually hone my problem-solving skills and algorithmic thinking.&lt;/p&gt;
&lt;p&gt;As I embark on my academic and research endeavors, I am eager to contribute to the ever-evolving landscape of technology and make meaningful strides in the fields that captivate my interest. My multifaceted interests, coupled with my practical experience in algorithmic trading, reflect not only my academic pursuits but also my commitment to staying at the forefront of innovation and applying my skills in real-world scenarios.&lt;/p&gt;
&lt;p&gt;I look forward to leveraging my academic foundation, research initiatives, and diverse interests to contribute meaningfully to the dynamic world of technology and make a positive impact.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>//localhost:1313/arawn/archives/</link>
        <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Projects</title>
        <link>//localhost:1313/arawn/projects/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/projects/</guid>
        <description>&lt;h1 id=&#34;adityas-projects-showcase&#34;&gt;
    &lt;a href=&#34;#adityas-projects-showcase&#34;&gt;#&lt;/a&gt;
    Aditya&amp;rsquo;s Projects Showcase
&lt;/h1&gt;&lt;h2 id=&#34;adityas-ravage-ecosystem&#34;&gt;
    &lt;a href=&#34;#adityas-ravage-ecosystem&#34;&gt;#&lt;/a&gt;
    Aditya&amp;rsquo;s Ravage Ecosystem
&lt;/h2&gt;&lt;h3 id=&#34;penetration-testing-tools&#34;&gt;
    &lt;a href=&#34;#penetration-testing-tools&#34;&gt;#&lt;/a&gt;
    Penetration Testing Tools
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Moonwalk-back&lt;/strong&gt;: Moonwalk-back is a penetration testing tool designed to leave zero traces on system logs, ensuring discreet testing activities.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Moonwalk-back&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Moonwalk-back&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Moonwalk-back&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Moonwalk-back Stars&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;post-exploitation-techniques&#34;&gt;
    &lt;a href=&#34;#post-exploitation-techniques&#34;&gt;#&lt;/a&gt;
    Post-Exploitation Techniques
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rusty-arsenal&lt;/strong&gt;: Rusty-arsenal leverages Rust programming language for post-exploitation techniques and process injection, providing robust security solutions.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Rusty-arsenal&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Rusty-arsenal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Rusty-arsenal&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Rusty-arsenal Stars&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;adityas-ml-ecosystem&#34;&gt;
    &lt;a href=&#34;#adityas-ml-ecosystem&#34;&gt;#&lt;/a&gt;
    Aditya&amp;rsquo;s ML Ecosystem
&lt;/h2&gt;&lt;h3 id=&#34;quantitative-finance&#34;&gt;
    &lt;a href=&#34;#quantitative-finance&#34;&gt;#&lt;/a&gt;
    Quantitative Finance
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantitative Finance with Backtesting Strategies&lt;/strong&gt;: This project focuses on implementing elegant and classic strategies for quantitative finance, facilitating reliable backtesting of trading strategies.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Quantfinance-with-backtesting&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Quantitative Finance Backtesting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Quantfinance-with-backtesting&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Quantitative Finance Backtesting Stars&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;virtual-try-on&#34;&gt;
    &lt;a href=&#34;#virtual-try-on&#34;&gt;#&lt;/a&gt;
    Virtual Try-On
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Virtual Try-On of Clothes&lt;/strong&gt;: Utilizing convolutional neural networks and recurrent neural networks, this project enables virtual try-on of clothes, enhancing the online shopping experience.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Try-on-of-clothes-using-CNN-RNN&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Virtual Try-On&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Try-on-of-clothes-using-CNN-RNN&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Virtual Try-On Stars&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;breast-cancer-detection-using-image-processing-different-approaches&#34;&gt;
    &lt;a href=&#34;#breast-cancer-detection-using-image-processing-different-approaches&#34;&gt;#&lt;/a&gt;
    Breast cancer detection using Image Processing different Approaches
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image Processing with Various Approaches&lt;/strong&gt;: Employing multiple techniques such as image processing and classifiers, this project aims to solve various image-related tasks.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Breast-cancer-detection-using-LS-MaskRCNN-and-DL&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Breast-Cancer-Predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Breast-cancer-detection-using-LS-MaskRCNN-and-DL&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Breast-Cancer-Predictions&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reinforcement-learning&#34;&gt;
    &lt;a href=&#34;#reinforcement-learning&#34;&gt;#&lt;/a&gt;
    Reinforcement Learning
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reinforcement Learning based on Stock Trading&lt;/strong&gt;: Using a DRQN model, this project applies reinforcement learning techniques to stock trading, aiming for optimized trading decisions.
&lt;ul&gt;
&lt;li&gt;GitHub Repository: &lt;a class=&#34;link&#34; href=&#34;https://github.com/Aditya-dom/Deep-Reinforcement-learning-stock-trading&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stock Trading using RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stars: &lt;img src=&#34;https://img.shields.io/github/stars/Aditya-dom/Deep-Reinforcement-learning-stock-trading&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Stock Trading RL Stars&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script src=&#34;https://giscus.app/client.js&#34;
        data-repo=&#34;Aditya-dom/arawn.github.io&#34;
        data-repo-id=&#34;R_kgDOLeAbmQ&#34;
        data-category=&#34;General&#34;
        data-category-id=&#34;DIC_kwDOLeAbmc4CeCQd&#34;
        data-mapping=&#34;title&#34;
        data-strict=&#34;0&#34;
        data-reactions-enabled=&#34;1&#34;
        data-emit-metadata=&#34;1&#34;
        data-input-position=&#34;top&#34;
        data-theme=&#34;dark_dimmed&#34;
        data-lang=&#34;en&#34;
        data-loading=&#34;lazy&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;</description>
        </item>
        <item>
        <title>Search</title>
        <link>//localhost:1313/arawn/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>//localhost:1313/arawn/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
